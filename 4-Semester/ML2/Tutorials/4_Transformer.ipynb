{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common imports\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Path variables\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "MODEL_PATH = os.path.join(PROJECT_ROOT_DIR, \"models\")\n",
    "DATA_PATH = os.path.join(PROJECT_ROOT_DIR, \"data\")\n",
    "logging_dir = os.path.join(PROJECT_ROOT_DIR, \"my_logs_3\")\n",
    "os.makedirs(MODEL_PATH, exist_ok=True)\n",
    "os.makedirs(DATA_PATH, exist_ok=True)\n",
    "\n",
    "# torch imports\n",
    "import torch\n",
    "# , torch.nn as nn\n",
    "# import torch.utils.data as data\n",
    "# import torch.nn.functional as F\n",
    "# import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 4: Fine-Tuning a pretrained Transformer-Model with Huggingface\n",
    "\n",
    "In this tutorial we use Huggingface for sentiment analysis of reviews: we use a pre-trained transformer model and fine-tune it for classifying [rotten tomatoe reviews](https://huggingface.co/datasets/rotten_tomatoes), see the documentation in the link.\n",
    "\n",
    "The model we want to use is: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"google-bert/bert-base-cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets evaluate transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"rotten_tomatoes\")\n",
    "\n",
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the text of the review of an instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tokenizer\n",
    "\n",
    "The first step for any NLP task is to use a tokenizer to transform text into a sequence of numbers. \n",
    "Also, you need strategies for truncation and padding for variable sequence lengths.\n",
    "\n",
    "Tokenizers are available on huggingface via transformers [AutoTokenizer](https://huggingface.co/docs/transformers/v4.41.0/en/model_doc/auto#transformers.AutoTokenizer)\n",
    "\n",
    "Find out how to get an instance of a pretrained tokenizer for the model we want to use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try the tokenizer out on the sentence \"I totally loved reading Harry Potter!\" and look at the output. \n",
    "Then turn this sequence into a sequence of words to find out how this tokenizer splits up the sentence!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `tokenize_fct(instances, max_length=512)` which applies the tokenizer to `instances`'s review-text, and cuts it to max_length, uses padding to size 512, and outputs the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now apply this function to the entire dataset while batching the data using [`.map`](https://huggingface.co/docs/datasets/v2.19.0/en/package_reference/main_classes#datasets.Dataset.map) and call the result `tokenized_datasets`\n",
    "For other preprocessing operations in huggingface, see: \n",
    "https://huggingface.co/docs/datasets/process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose a smaller subset of the datasets to reduce the amount of training time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the pretrained model \"google-bert/bert-base-cased\" for Sequence Classification via [`AutoModelForSequenceClassification`](https://huggingface.co/docs/transformers/v4.41.0/en/model_doc/auto#transformers.AutoModelForSequenceClassification) and specify the necessary number of labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to fine-tune this model. Do the following: \n",
    "- specify training hyperparameters using [`Training_Arguments`](https://huggingface.co/docs/transformers/v4.41.0/en/main_classes/trainer#transformers.TrainingArguments): specify the output-directory as `MODEL_PATH` as `finetuned_hf_model`, and the `eval_strategy` to \"epoch\" and `num_train_epochs`=20, and `optim='adamw_torch'`\n",
    "- load the accuracy metric from the [`evaluate`](https://huggingface.co/docs/evaluate/index) library\n",
    "- write a function `compute_metric(logits, labels)` that computes the metric based on the true labels and the prediction logits, where the logits are the outputs of the last classification layer BEFORE the Softmax, so you have to compute the argument with the maximal entry.\n",
    "- create a [`Trainer`](https://huggingface.co/docs/transformers/v4.41.0/en/main_classes/trainer#transformers.Trainer) object `trainer` with all necessary information above \n",
    "- and train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, evaluate the trained model on the eval_dataset using `trainer`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theoretical Questions\n",
    "\n",
    "**Question 1** \n",
    "Describe all three variants of attention used in an encoder-decoder transformer and where in the model it is used. \n",
    "\n",
    "#TODO\n",
    "\n",
    "**Question 2**\n",
    "Desribe the method of an autoregressive encoder-decoder architecture in the example of a seq-to-seq RNN. \n",
    "\n",
    "#TODO\n",
    "\n",
    "**Question 3**\n",
    "What is positional encoding and why is it necessary? \n",
    "\n",
    "#TODO\n",
    "\n",
    "**Question 4**\n",
    "What is teacher forcing and when/why is it used?\n",
    "\n",
    "#TODO\n",
    "\n",
    "**Question 5**\n",
    "Describe parametric Cross-Attention. \n",
    "\n",
    "#TODO\n",
    "\n",
    "**Question 6**\n",
    "Describe parametric Cross-Attention. \n",
    "\n",
    "#TODO\n",
    "\n",
    "**Question 6**\n",
    "Suppose the input of a non-parametric ross-Attention Layer is 20-dimensional, keys are 20-dimensional, and values are 30-dimensional. What is the dimension of the output?\n",
    "\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
