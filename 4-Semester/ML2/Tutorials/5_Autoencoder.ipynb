{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practicals - Pytorch: Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common imports\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Path variables\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "MODEL_PATH = os.path.join(PROJECT_ROOT_DIR, \"models\")\n",
    "DATA_PATH = os.path.join(PROJECT_ROOT_DIR, \"data\")\n",
    "logging_dir = os.path.join(PROJECT_ROOT_DIR, \"my_logs_4\")\n",
    "os.makedirs(MODEL_PATH, exist_ok=True)\n",
    "os.makedirs(DATA_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch imports\n",
    "import torch, torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Torchvision\n",
    "import torchvision\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        \n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoders - Introduction\n",
    "\n",
    "In this tutorial, we will take a closer look at autoencoders (AE). \n",
    "\n",
    "An Autoencoder consists of: \n",
    "- an encoder, which encodes an image $x$ to a smaller-dimensional so-called \"latent vector\" $z$. \n",
    "- a decoder, which takes this latent vector $z$ and recreates the image $x$. \n",
    "\n",
    "Why? The encoding $z$ of $x$ contains all the information necessary to reconstruct $x$ (i.e. everything important), so it has to learn the features of the image!\n",
    "\n",
    "Advantage of Autoencoder: It is an unsupervised method! You can use it even if you don't have labels. \n",
    "\n",
    "i.e.: if you want to use a CNN to learn features, but don't have labels to train it on, use an Autoencoder."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset \n",
    "\n",
    "We again work with the CIFAR10 dataset. In CIFAR10, each image has 3 color channels and is 32x32 pixels large. \n",
    "\n",
    "In contrast to the previous CIFAR10 tutorial, we do not normalize the data explicitly with a mean of 0 and std of 1, but roughly estimate it scaling the data between -1 and 1. This is because limiting the range will make our task of predicting/reconstructing images easier."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell: \n",
    "- define a transform that turns the input to a tensor and then normalizes it, where we assume the mean and standard deviation are 0.5 in all channels\n",
    "- load the CIFAR10 training set with this transform and split in train and val set [45000, 5000] like before \n",
    "- load the test set with the same transform\n",
    "- define `train_loader`, `val_loader`, and `test_loader` with batch size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's draw some of the training data\n",
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "\n",
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "  plt.subplot(2,3,i+1)\n",
    "  plt.tight_layout()\n",
    "  plt.imshow(example_data[i].permute(1, 2, 0))\n",
    "  plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the AE\n",
    "\n",
    "The autoencoder consists of an **encoder** that maps the input $x$ to a lower-dimensional feature vector $z$, and a **decoder** that reconstructs the input $\\hat{x}$ from $z$. We train the model by comparing $x$ to $\\hat{x}$ and optimizing the parameters to minimize the MSE between these."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First Step: The Encoder\n",
    "\n",
    "Note: we do not apply BatchNormlization here. This is because we want the encoding of each image to be independent of all other images. Otherwise, we might introduce correlations into the encoding or decoding that we do not want to have. So: best practice = No BatchNormalization in Autoencoders! Instead, if you want to Normalize, use e.g. LayerNormalization. Here, the model is so small we don't need normalization. \n",
    "\n",
    "In the following cell, complete the code that defined the AE encoder class:\n",
    "- self.net consists of the following modules types:\n",
    "    - module type 1: 3x3 convolution with stride 2 and padding = 1 and + activation function\n",
    "    - module type 2: 3x3 same convolution with stride 1 + activation function\n",
    "- in the following order: \n",
    "    - type 1(c_hid out channels)\n",
    "    - type 2(2*c_hid out channels)\n",
    "    - type 1(2* c_hid out channels) \n",
    "    - type 2(2* c_hid out channels) \n",
    "    - type 1(2* c_hid out channels)\n",
    "    - flatten\n",
    "    - Linear with output-dimension=latent_dim\n",
    "  \n",
    "- Define the paramter initialization `_init_params()` as follows (you can e.g. take a look at how it was done in the GoogleNet class in the last practicals...):\n",
    "for the modules in `self.net`, use Xavier Normal (= Glorot) initialization for the Linear Layer and Kaiming Normal (=He) initialization for the Conv2d layers\n",
    "\n",
    "- forward is just net applied to the input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 num_input_channels : int, \n",
    "                 base_channel_size : int, \n",
    "                 latent_dim : int, \n",
    "                 act_fn : object = nn.GELU):\n",
    "        \"\"\"\n",
    "        Inputs: \n",
    "            - num_input_channels : Number of input channels of the image. For CIFAR, this parameter is 3\n",
    "            - base_channel_size : Number of channels we use in the first convolutional layers. Deeper layers might use a duplicate of it.\n",
    "            - latent_dim : Dimensionality of latent representation z\n",
    "            - act_fn : Activation function used throughout the encoder network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        c_hid = base_channel_size\n",
    "        self.net = nn.Sequential(\n",
    "            #TODO\n",
    "        )\n",
    "\n",
    "        self._init_params()\n",
    "\n",
    "    def _init_params(self):\n",
    "        #TODO\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** As always, write down dimensions and numbers of parameters for the Encoder above in the table below!\n",
    "`\n",
    "**Answer:** \n",
    "\n",
    "Input: 3x32x32\n",
    "\n",
    "|Layer|Layer-Output-Dimension|Weights|Biases|\n",
    "|:---|:---|:---|:---|\n",
    "|Conv2d 1|x|x|x|\n",
    "|Conv2d 2|x|x|x|\n",
    "|Conv2d 3|x|x|x|\n",
    "|Conv2d 4|x|x|x|\n",
    "|Conv2d 5|x|x|x|\n",
    "|Flatten|x|x|x|\n",
    "|Linear|x|x|x|\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second Step: The Decoder\n",
    "\n",
    "The decoder is a mirrored, flipped version of the encoder. The only difference is that we replace strided convolutions (type 1 above) by transposed convolutions (i.e. deconvolutions) `nn.ConvTranspose2d` with stride=2 to upscale the features in a \"symmetric\" way. \n",
    "\n",
    "Construct the Decoder `self.net` as almost the mirror of the Encoder `self.net`: \n",
    "- the decoder self.net consists of the following modules types:\n",
    "    - module type 1: 3x3 same **transpose convolution** with stride 2, padding = 1 and and `output_padding=1`  + activation function \n",
    "    - module type 2: 3x3 same **convolution** with stride 1 + activation function\n",
    "- in the following order: \n",
    "    - type 1(2*c_hid out channels)\n",
    "    - type 2(2*c_hid out channels)\n",
    "    - type 1(c_hid out channels) \n",
    "    - type 2(c_hid out channels) \n",
    "    - type 1(num_input_channels out channels)\n",
    "    - Tanh\n",
    "\n",
    "Construct the Decoder `self.linear` as the \"reverse\" Linear Layer (i.e. input and output dimension reversed) of the Encoder `self.linear` + `act_fn()`\n",
    "\n",
    "\n",
    "Define the forward function as: \n",
    "- linear, \n",
    "- then reshape to batch x channel x HxB, where HxB=4x4, \n",
    "- then net. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 num_input_channels : int, \n",
    "                 base_channel_size : int, \n",
    "                 latent_dim : int, \n",
    "                 act_fn : object = nn.GELU):\n",
    "        \"\"\"\n",
    "        Inputs: \n",
    "            - num_input_channels : Number of channels of the image to reconstruct. For CIFAR, this parameter is 3\n",
    "            - base_channel_size : Number of channels we use in the last convolutional layers. Early layers might use a duplicate of it.\n",
    "            - latent_dim : Dimensionality of latent representation z\n",
    "            - act_fn : Activation function used throughout the decoder network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        c_hid = base_channel_size\n",
    "        self.linear = nn.Sequential(\n",
    "            #TODO\n",
    "        )\n",
    "        self.net = nn.Sequential(\n",
    "            #TODO\n",
    "        )\n",
    "\n",
    "        self._init_params()\n",
    "\n",
    "    def _init_params(self):\n",
    "        #TODO\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Write down dimensions and numbers of parameters for the Decoder above in the table below!\n",
    "\n",
    "**Answer:** \n",
    "\n",
    "|Layer|Layer-Output-Dimension|Weights|Biases|\n",
    "|:---|:---|:---|:---|\n",
    "|Linear|x|x|x|\n",
    "|TransposeConv2d 1|x|x|x|\n",
    "|Conv2d 1|x|x|x|\n",
    "|TransposeConv2d 2|x|x|x|\n",
    "|Conv2d 2|x|x|x|\n",
    "|TransposeConv2d 3|x|x|x|\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Putting it all together\n",
    "\n",
    "Finally, combine the encoder and decoder together into the autoencoder architecture: \n",
    "Define the forward function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 base_channel_size: int, \n",
    "                 latent_dim: int, \n",
    "                 encoder_class : object = Encoder,\n",
    "                 decoder_class : object = Decoder,\n",
    "                 num_input_channels: int = 3, \n",
    "                 width: int = 32, \n",
    "                 height: int = 32):\n",
    "        super().__init__()\n",
    "        # Creating encoder and decoder\n",
    "        self.encoder = encoder_class(num_input_channels, base_channel_size, latent_dim)\n",
    "        self.decoder = decoder_class(num_input_channels, base_channel_size, latent_dim)\n",
    "        # Example input array needed for visualizing the graph of the network\n",
    "        self.example_input_array = torch.zeros(2, num_input_channels, width, height)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The forward function takes in an image and returns the reconstructed image\n",
    "        \"\"\"\n",
    "        #TODO\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Autoencoder\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "For the loss function, we use the mean squared error (MSE). \n",
    "Recall: MSE computes the total squared error for each instance (an instance is an image here!) and then computes the mean only over the batch size. \n",
    "\n",
    "This is why we can't simply use the mse_loss function on the entire batch of size [Batch-size, height, width, channels], because it would compute the mean squared error of each pixel, i.e. over all dimensions in [Batch-size, height, width, channels], not just over batch-size. \n",
    "\n",
    "\n",
    "The following function `reconstruction_loss` fixes that: \n",
    "`mse_loss` with `reduction=\"none\"` computes the absolute sqared error (without the mean) over all 4 tensor dimensions.\n",
    "Then you sum up the absolute errors for each image (dim 1,2,3) and only compute the mean over batch size (dim 0). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruction_loss(x, x_hat):\n",
    "        loss = F.mse_loss(x, x_hat, reduction=\"none\")\n",
    "        loss = loss.sum(dim=[1,2,3]).mean(dim=[0])\n",
    "        return loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Eval Functions\n",
    "\n",
    "We use a modified version of the well-known training and eval functions: \n",
    "- this time, without accuracy, since we want to reconstruct the images, not classify them\n",
    "- with the loss_module replaced by the custom function `reconstruction_loss`.  \n",
    "- also, this time, we don't need the CIFAR10 labels as labels, but since we want to reconstruct the images, we need input = label!\n",
    "\n",
    "If you want to do training in a more \"sleek\" way in the future with pre-implemented callbacks for TensorFlow, you could read into [Pytorch Lightning](https://www.pytorchlightning.ai/index.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tensorboard logger from PyTorch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def train_model_with_logger(model, optimizer, scheduler, data_loader, val_loader, num_epochs=50, logging_dir=logging_dir):\n",
    "    # Create TensorBoard logger\n",
    "    writer = SummaryWriter(logging_dir)\n",
    "    model_plotted = False\n",
    "\n",
    "\n",
    "    val_scores = []\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # Set model to train mode\n",
    "        model.train()\n",
    "        \n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for data_inputs, _ in data_loader:\n",
    "            \n",
    "            ## Step 1: Move input data to device (only strictly necessary if we use GPU)\n",
    "            data_inputs = data_inputs.to(device)\n",
    "\n",
    "            # For the very first batch, we visualize the computation graph in TensorBoard\n",
    "            if not model_plotted:\n",
    "                writer.add_graph(model, data_inputs)\n",
    "                model_plotted = True\n",
    "\n",
    "            ## Step 2: Run the model on the input data\n",
    "            preds = model(data_inputs)\n",
    "\n",
    "            ## Step 3: Calculate the loss \n",
    "            loss = reconstruction_loss(preds, data_inputs)\n",
    "            \n",
    "            ## Step 4: Perform backpropagation\n",
    "            # Before calculating the gradients, we need to ensure that they are all zero.\n",
    "            # The gradients would not be overwritten, but actually added to the existing ones.\n",
    "            optimizer.zero_grad()\n",
    "            # Perform backpropagation\n",
    "            loss.backward()\n",
    "\n",
    "            ## Step 5: Update the parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            ## Step 6: Take the running average of loss and update true\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        \n",
    "        # Validation at the end of training: \n",
    "        val_loss = eval_model(model, val_loader)\n",
    "\n",
    "        ## Scheduler Step: Do one LR scheduler step\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        # Add average loss to TensorBoard\n",
    "        epoch_loss /= len(data_loader)\n",
    "        writer.add_scalar('training_loss',\n",
    "                          epoch_loss,\n",
    "                          global_step = epoch + 1)\n",
    "        \n",
    "        \n",
    "        # Add validation loss to TensorBoard\n",
    "        writer.add_scalar('validation_loss',\n",
    "                          val_loss,\n",
    "                          global_step = epoch + 1)\n",
    "        \n",
    "        \n",
    "        # Produce the output\n",
    "        print(f'''Epoch: {epoch} Training loss: {epoch_loss:.2f} ''')\n",
    "\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader):\n",
    "    model.eval() # Set model to eval mode\n",
    "    loss = 0.0\n",
    "\n",
    "    with torch.no_grad(): # Deactivate gradients for the following code\n",
    "        for data_inputs, _ in data_loader:\n",
    "\n",
    "            # Determine prediction of model on dev set\n",
    "            data_inputs = data_inputs.to(device)\n",
    "            preds = model(data_inputs)\n",
    "\n",
    "            # Compute the batch's loss\n",
    "            loss += reconstruction_loss(preds, data_inputs).item()\n",
    "\n",
    "    loss /= len(data_loader)\n",
    "    return loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model \n",
    "\n",
    "For latent dimension = 128 create an instance `model_128` of the Autoencoder, push the model to the device and train it for 20 epochs. \n",
    "\n",
    "Configure the training of each model by defining: \n",
    "- the optimizer to be Adam with lr=1e-3\n",
    "- the scheduler to be ReduceLROnPlateau with factor 0.2, patience=5, min_lr=5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model(model_128, val_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating new images with an Autoencoder?\n",
    "\n",
    "Recall: An Autoencoder consists of\n",
    "- an encoder, which encodes an image $x$ to a smaller-dimensional so-called \"latent vector\" $z$. \n",
    "- a decoder, which takes this latent vector $z$ and recreates the image $x$. \n",
    "\n",
    "Idea: Can you train an autoencoder and use the decoder to generate random images (similarly to Midjourney) from a random latent vector $z$, i.e. a random vector of dimension = `latent_dim`? Let's find out below:\n",
    "\n",
    "- we create a random `latent_vector` from the 128-dimensional latent space\n",
    "- with `torch.no_grad()` (since we don't need gradients): apply the decoder of `model_128` to these `latent_vectors` and push them to the cpu. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_vector = torch.randn(1, 128, device=device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    img = model_128.decoder(latent_vector)\n",
    "    img = img.squeeze().cpu()\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.imshow(img.permute(1, 2, 0))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the generated images are not realistic images. As the autoencoder was allowed to structure the latent space in whichever way it suits the reconstruction best, there is no incentive to map every possible latent vector to realistic images. However, if you structure the latent space to learn the underlying probability distribution of the images, then you could do exactly what we tried to do above! This is a variant of the autoencoder called a **\"Variational Autoencoder\"**. It uses a statistical model on top of the Autoencoder, and is able to generate new samples."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding visually similar images\n",
    "\n",
    "One application of autoencoders is to build an image-based search engine to retrieve visually similar images. This can be done by representing all images as their latent dimensionality, and find the closest $K$ images in this domain. The first step to such a search engine is to encode all images into $z$. In the following, we will use the training set as a search corpus, and the test set as queries to the system."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `embed_imgs(model, data_loader)` that encodes all images in data_loader using our model_128, and return both image and encoding as lists `img_list` and `embed_list`: \n",
    "- create empty lists `img_list` and `embed_list`\n",
    "- we don't train, so put the model in eval-mode \n",
    "- cycle through batches from the data_loader as usual\n",
    "- with torch.no_grad(): push images to device and compute the embedding of these images with model.encoder\n",
    "- add the images to img_list and the encodings to the embed_list \n",
    "- return torch.cat(list, dim_0) for both lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_imgs(model, data_loader):\n",
    "    # Encode all images in the data_laoder using model, and return both images and encodings\n",
    "    img_list, embed_list = [], []\n",
    "    model.eval()\n",
    "    for imgs, _ in data_loader:\n",
    "        with torch.no_grad():\n",
    "            z = model.encoder(imgs.to(device))\n",
    "        img_list.append(imgs)\n",
    "        embed_list.append(z)\n",
    "        print(img_list, \"Embed\", embed_list)\n",
    "    return (torch.cat(img_list, dim=0), torch.cat(embed_list, dim=0))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply it to both train_loader and test_loader to generate train_img_embeds and test_img_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_embeds = embed_imgs(model_128, train_loader)\n",
    "test_img_embeds = embed_imgs(model_128, test_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have images and encodings in lists for both train and test data. Below is a function that finds the $K$ closest images called `find_similar_images(query_img, query_z, key_embeds, K=8)`: \n",
    "- it computes the distance between the query-z and the key_embeds in the batch with torch.cdist(p=2) - this distance function computes the distance with respect to $p$-norm (for p=2 it's just ordinary Euclidean distance) for an entire batch. \n",
    "- then it sorts the distances with torch.sort and plot the K closest images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_images(query_img, query_z, key_embeds, K=8):\n",
    "    # Find closest K images. We use the euclidean distance here but other like cosine distance can also be used.\n",
    "    dist = torch.cdist(query_z[None,:], key_embeds[1], p=2)\n",
    "    dist = dist.squeeze(dim=0)\n",
    "    dist, indices = torch.sort(dist)\n",
    "    # Plot K closest images\n",
    "    imgs_to_display = torch.cat([query_img[None], key_embeds[0][indices[:K]]], dim=0)\n",
    "    grid = torchvision.utils.make_grid(imgs_to_display, nrow=K+1, normalize=True, range=(-1,1))\n",
    "    grid = grid.permute(1, 2, 0)\n",
    "    plt.figure(figsize=(12,3))\n",
    "    plt.imshow(grid)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply this function to the first 8 test_img_embeds-images and encodings with key_embeds= train_img_embeds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the closest images for the first N test images as example\n",
    "for i in range(8):\n",
    "    find_similar_images(test_img_embeds[0][i], test_img_embeds[1][i], key_embeds=train_img_embeds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outlook: If you want to see how important initialization is for NNs, play around with the initializations or do not initialize at all and see what happens."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "6c689652473bf5d24b7e8082146cd0e239d6c11b655d160204aff73691e33248"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
