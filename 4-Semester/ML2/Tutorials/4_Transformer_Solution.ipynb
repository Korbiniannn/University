{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common imports\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Path variables\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "MODEL_PATH = os.path.join(PROJECT_ROOT_DIR, \"models\")\n",
    "DATA_PATH = os.path.join(PROJECT_ROOT_DIR, \"data\")\n",
    "logging_dir = os.path.join(PROJECT_ROOT_DIR, \"my_logs_3\")\n",
    "os.makedirs(MODEL_PATH, exist_ok=True)\n",
    "os.makedirs(DATA_PATH, exist_ok=True)\n",
    "\n",
    "# torch imports\n",
    "import torch\n",
    "# , torch.nn as nn\n",
    "# import torch.utils.data as data\n",
    "# import torch.nn.functional as F\n",
    "# import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 4: Fine-Tuning a pretrained Transformer-Model with Huggingface\n",
    "\n",
    "In this tutorial we use Huggingface for sentiment analysis of reviews: we use a pre-trained transformer model and fine-tune it for classifying [rotten tomatoe reviews](https://huggingface.co/datasets/rotten_tomatoes), see the documentation in the link.\n",
    "\n",
    "The model we want to use is: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"google-bert/bert-base-cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets evaluate transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"rotten_tomatoes\")\n",
    "\n",
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the text of the review of an instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"][0][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tokenizer\n",
    "\n",
    "The first step for any NLP task is to use a tokenizer to transform text into a sequence of numbers. \n",
    "Also, you need strategies for truncation and padding for variable sequence lengths.\n",
    "\n",
    "Tokenizers are available on huggingface via transformers [AutoTokenizer](https://huggingface.co/docs/transformers/v4.41.0/en/model_doc/auto#transformers.AutoTokenizer)\n",
    "\n",
    "Find out how to get an instance of a pretrained tokenizer for the model we want to use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try the tokenizer out on the sentence \"I totally loved reading Harry Potter!\" and look at the output. \n",
    "Then turn this sequence into a sequence of words to find out how this tokenizer splits up the sentence!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_seq = tokenizer(\"I totally loved the movie Harry Potter!\")\n",
    "print(id_seq)\n",
    "print(tokenizer.convert_ids_to_tokens(id_seq.input_ids))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `tokenize_fct(instances, max_length=512)` which applies the tokenizer to `instances`'s review-text, and cuts it to max_length, uses padding to size 512, and outputs the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_fct(instances, max_length=512):\n",
    "    model_inputs = tokenizer(\n",
    "        instances[\"text\"],\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding='max_length'\n",
    "    )\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now apply this function to the entire dataset while batching the data using [`.map`](https://huggingface.co/docs/datasets/v2.19.0/en/package_reference/main_classes#datasets.Dataset.map) and call the result `tokenized_datasets`\n",
    "For other preprocessing operations in huggingface, see: \n",
    "https://huggingface.co/docs/datasets/process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = dataset.map(tokenize_fct, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose a smaller subset of the datasets to reduce the amount of training time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(small_train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the pretrained model for Sequence Classification we want via [`AutoModelForSequenceClassification`](https://huggingface.co/docs/transformers/v4.41.0/en/model_doc/auto#transformers.AutoModelForSequenceClassification) and specify the number of labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to fine-tune this model. Do the following: \n",
    "- specify training hyperparameters using [`Training_Arguments`](https://huggingface.co/docs/transformers/v4.41.0/en/main_classes/trainer#transformers.TrainingArguments): specify the output-directory as `MODEL_PATH` as `finetuned_hf_model`, and the `eval_strategy` to \"epoch\" and `num_train_epochs`=20, and `optim='adamw_torch'`\n",
    "- load the accuracy metric from the [`evaluate`](https://huggingface.co/docs/evaluate/index) library\n",
    "- write a function `compute_metric(logits, labels)` that computes the metric based on the true labels and the prediction logits, where the logits are the outputs of the last classification layer BEFORE the Softmax, so you have to compute the argument with the maximal entry.\n",
    "- create a [`Trainer`](https://huggingface.co/docs/transformers/v4.41.0/en/main_classes/trainer#transformers.Trainer) object `trainer` with all necessary information above \n",
    "- and train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(output_dir=os.path.join(MODEL_PATH, \"finetuned_hf_model\"), overwrite_output_dir=True, evaluation_strategy=\"epoch\", num_train_epochs=20, optim='adamw_torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, evaluate the trained model on the eval_dataset using `trainer`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theoretical Questions\n",
    "\n",
    "\n",
    "**Question 1** \n",
    "Describe all three variants of attention used in an encoder-decoder transformer and where in the model it is used. \n",
    "\n",
    "see lecture\n",
    "\n",
    "**Question 2**\n",
    "Desribe the method of an autoregressive encoder-decoder architecture in the example of a seq-to-seq RNN. \n",
    "\n",
    "see lecture\n",
    "\n",
    "**Question 3**\n",
    "What is positional encoding and why is it necessary? \n",
    "\n",
    "see lecture\n",
    "\n",
    "**Question 4**\n",
    "What is teacher forcing and when/why is it used?\n",
    "\n",
    "see lecture\n",
    "\n",
    "**Question 6**\n",
    "Describe parametric Cross-Attention. \n",
    "\n",
    "see lecture\n",
    "\n",
    "**Question 6**\n",
    "Suppose the input of a non-parametric ross-Attention Layer is 20-dimensional, keys are 20-dimensional, and values are 30-dimensional. What is the dimension of the output?\n",
    "\n",
    "30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
