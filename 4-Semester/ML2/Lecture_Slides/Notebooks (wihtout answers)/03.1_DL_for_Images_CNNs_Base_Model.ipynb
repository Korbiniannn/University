{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/ml/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/opt/homebrew/anaconda3/envs/ml/lib/python3.9/site-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (5.1.0)/charset_normalizer (2.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.16.1\n"
     ]
    }
   ],
   "source": [
    "# Setup for Keras\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras #requirement: keras 3\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "#os.environ[\"KERAS_BACKEND\"] = \"pytorch\"\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "keras.utils.set_random_seed(42)\n",
    "\n",
    "print(tf.__version__) #requirement: >= 15\n",
    "\n",
    "# Where to save the models\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "MODEL_PATH = os.path.join(PROJECT_ROOT_DIR, \"models\")\n",
    "os.makedirs(MODEL_PATH, exist_ok=True)\n",
    "\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Utility functions to plot grayscale and RGB\n",
    "def plot_image(image):\n",
    "    plt.imshow(image, cmap=\"gray\", interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "def plot_color_image(image):\n",
    "    plt.imshow(image, interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3. DL for Images: CNNs/ConvNets\n",
    "\n",
    "In 2009, the Imagenet Database was published. By now, it consists of 14 Mio. images in 20,000 categories which were hand-annotated.\n",
    "\n",
    "<img src=\"../../assets/Image_ImageNet.jpg\" alt=\"Image_ImageNet.jpg\" style=\"width: 500px\" title=\"ImageNet\"/><br></br>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3.1 CNN/ConvNet Base Model\n",
    "\n",
    "## Agenda\n",
    "\n",
    "0. Data Augmentaion for images\n",
    "1. Introduction: From MLPs to CNNs, and from classical filters to trained ones\n",
    "2. Convolution to extract image features\n",
    "3. Variants of convolution: \n",
    "   1. Same convolution\n",
    "   2. Convolution with stride\n",
    "   3. Dilated convolution\n",
    "   4. Deconvolution = Transpose convolution\n",
    "   5. 3d- and 1d- convolution\n",
    "4. Pooling Layers: maxpool, avgpool\n",
    "5. Architecture of a CNN\n",
    "6. Case Study: AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.1.0 Data Augmentation (esp. for images)\n",
    "\n",
    "**Data augmentation** (Krizhevsky 12) = Increasing the amount and variety of the training data by varying the given training instances to produce new ones (that are still viable). In particular for images: flip, crop, rotate resize, shift, tint, change contrast: \n",
    "\n",
    "<br></br><img src=\"../../assets/Image_050_Image_Augmentation.png\" alt=\"Data Augmentation\" width=\"400\" title = \"Hands-on-ML\"/>   <br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br></br><img src=\"../../assets/Image_Augmentation_Frog_Original.png\" alt=\"Data Augmentation Frog Original\" width=\"200\" title = \"UVA DL Course\"/>   <img src=\"../../assets/Image_Augmentation_Frog_Flip.png\" alt=\"Data Augmentation Frog Flip\" width=\"200\" title = \"UVA DL Course\"/>  \n",
    "<br></br><img src=\"../../assets/Image_Augmentation_Frog_RandomCrop.png\" alt=\"Data Augmentation Frog Crop\" width=\"200\" title = \"UVA DL Course\"/>  <img src=\"../../assets/Image_Augmentation_Frog_Tint.png\" alt=\"Data Augmentation Frog Tint\" width=\"200\" title = \"UVA DL Course\"/>   <br></br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.1.1. Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**The \"basic\" tasks for image data**\n",
    "\n",
    "**Image Classification:**\n",
    "\n",
    "<img src=\"../../assets/Image_Image_Classification_Task.jpg\" alt=\"Image_Image_Classification_Task.jpg\" style=\"width: 500px\" title = \"UDL\"/>   <br></br>\n",
    "\n",
    "**Regression Tasks like Depth Extimation:**\n",
    "\n",
    "<img src=\"../../assets/Image_Depth_Estimation_Task.jpg\" alt=\"Image_Depth_Estimation_Task.jpg\" style=\"width: 500px\" title = \"UDL\"/>   <br></br>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**What makes images different? **\n",
    "\n",
    "<img src=\"../../assets/Image_Tiger_in_Water.jpg\" alt=\"Image_Tiger_in_Water.jpg\" style=\"width: 500px\" title=\"https://www.wallpaperflare.com/tiger-pc-backgrounds-hd-water-animal-animal-themes-animal-wildlife-wallpaper-qmanr/download\"/>\n",
    "\n",
    "- huge **input dimensionality**... $1920\\times 1080\\times 3=6,220,800$ input variables!!!! \n",
    "- comparison: 1-layer NN with only 1000 neurons $\\rightarrow$ 200 mio parameters $\\Rightarrow$ a normal MLP would have far too many parameters to be trainable!\n",
    "- variances can change the input vectors significantly but don't change the meaning of a picture: "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Depth and point of view**\n",
    "\n",
    "<img src=\"../../assets/Image_Tiger_in_Water_Depth.png\" alt=\"Image_Tiger_in_Water_Depth.png\" style=\"width: 500px\" title=\"https://www.wallpaperflare.com/tiger-pc-backgrounds-hd-water-animal-animal-themes-animal-wildlife-wallpaper-qmanr/download\"/>\n",
    "\n",
    "Images only depict two dimensions, when the image content is 3-dimensional! Small changes of point of view totally change the pixel values. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Shift/Translation**\n",
    "\n",
    "<img src=\"../../assets/Image_Tiger_in_Water_Shifted.png\" alt=\"Image_Tiger_in_Water_Shifted.png\" style=\"width: 500px\" title=\"https://www.wallpaperflare.com/tiger-pc-backgrounds-hd-water-animal-animal-themes-animal-wildlife-wallpaper-qmanr/download\"/>\n",
    "\n",
    "This image has been shifted slightly! The first 5x5 values for original image (left) and the shifted image (right) are quite different!\n",
    "<br></br><img src=\"../../assets/Image_Tiger_in_Water_first_5times5.png\" alt=\"Image_Tiger_in_Water_first_5times5.png\" style=\"height: 65px\" /><img src=\"../../assets/Image_Tiger_in_Water_Shifted_first_5times5.png\" alt=\"Image_Tiger_in_Water_Shifted_first_5times5.png\" style=\"height: 65px\" />\n",
    "\n",
    "**Size of pixel values** Small visual changes (even if invisible to the naked eye) like picture temperature, colour tone, colour saturation make big changes to the size of pixel values!\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**What a NN for images should be able to deal with:**\n",
    "\n",
    "1. spatial structure information + translation invariance\n",
    "2. huge input dimensionalities (i.e. reduce number of parameters)\n",
    "3. local variances (e.g. some \"bad\" pixels on an image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**From MLPs to NNs for Images**\n",
    "\n",
    "Data for MLPs were 1-d vectors and each layer a 1-d vector of neurons.  \n",
    "\n",
    "Black-and-white images are matrices, i.e. 2-d objects = 2-d layers of neurons:\n",
    "\n",
    "<img src=\"../../assets/Image_Neurons.png\" width=\"250\" title = \"Murphy\"/>   <br></br>\n",
    "\n",
    "RGB images are 3 stacked matrices, i.e. a 3-d object, a so-called **3d-tensor**. The stacked matrices in a 3d-tensor are called **channels**: \n",
    "\n",
    "<img src=\"../../assets/Image_Neurons_channels.png\" width=\"250\" title = \"Murphy\"/>   <br></br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Features for Images**\n",
    "\n",
    "- For tabular data, a **feature** is one (meaningful) input dimension\n",
    "- For images, a **feature** isn't one pixel value, since it doesn't contain meaning, but: a piece of information about the content of an image; typically about whether a certain region of the image has certain properties. Features may be specific structures in the image such as points, edges or objects. \n",
    "\n",
    "**Aim:** Extract these features with a NN. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A NN for images doesn't consist of functions between 1d-layers of neurons (left), but 3d-layers of neurons (right): \n",
    "\n",
    "\n",
    "<img src=\"../../assets/MLP_vs_CNN.jpg\" width=\"500\" title = \"Murphy\"/>   <br></br>\n",
    "\n",
    "What we need to determine is what the function $f$ should look like. (it shouldn't contain too many parameters)!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "You can think of image features like things you can detect with some sort of template (Schablone) by sliding it across all positions of an image. $\\rightarrow$ **filters or kernels**. \n",
    "\n",
    "<img src=\"../../assets/Image_Grumpy_Cat_Ears.png\" alt=\"Image_Grumpy_Cat_Ears.png\" style=\"width: 200px\"/><img src=\"../../assets/Image_Grumpy_Cat_Head.png\" alt=\"Image_Grumpy_Cat_Head.png\" style=\"width: 200px\"/><img src=\"../../assets/Image_Grumpy_Cat_Texture.png\" alt=\"Image_Grumpy_Cat_Texture.png\" style=\"width: 200px\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Classical Filters**\n",
    "\n",
    "You already know **filters or kernels** from \"Bildverarbeitung\": e.g. \n",
    "- Sobel filters extracting edges\n",
    "- Gaussian filter for blurring and noise reduction\n",
    "  \n",
    "**Question:** How are these filters applied to an image?\n",
    "\n",
    "See this animation for an example: https://en.wikipedia.org/wiki/Kernel_(image_processing)#/media/File:2D_Convolution_Animation.gif"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**From Classical Kernels to NNs**\n",
    "\n",
    "**Question:** How should we choose the filters to extract features in a NN?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Idea:** the map in an Image NN could be convolution with kernels/filters: \n",
    "\n",
    "<img src=\"../../assets/Convolution_Layer_Units.jpg\" width=\"350\" title = \"Murphy\"/>   <br></br>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.1.2. Convolution (Conv2D) to extract image features\n",
    "\n",
    "**Convolution (Conv2D) with one channel (e.g. BW):**\n",
    "Let $\\mathbf{W}$ be a filter/kernel of size $H_W\\times B_W$, and $\\mathbf{X}\\in \\mathbb{R}^{H\\times B}$ an image matrix. Then **the convolution of $\\mathbf{X}$ with $\\mathbf{W}$** is defined as the $(?)\\times (?)$-matrix $\\mathbf{X}\\circledast\\mathbf{W}$ with $i,j$-th entry (=similarity measure for the section of the image whose left upper corner is at $x_{ij}$):\n",
    "$$(\\mathbf{X}\\circledast\\mathbf{W})_{ij}=\\sum_{h=0}^{H_W-1}\\sum_{b=0}^{B_W-1}x_{i+h,j+b}\\cdot w_{h,b}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Example:**\n",
    "\n",
    "<img src=\"../../assets/Image_CNN_Convolution.png\" alt=\"convolution\" width=\"350\" title = \"Murphy\"/>   <br></br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Question:** Compute $A\\circledast F, B\\circledast F$ for \n",
    "$F=\\left(\\begin{array}{cc}\n",
    "1&0\\\\\n",
    "0&1\n",
    "\\end{array}\\right), A= \\left(\\begin{array}{cc}\n",
    "1&0\\\\\n",
    "0&1\n",
    "\\end{array}\\right), B=\\left(\\begin{array}{cc}\n",
    "0&1\\\\\n",
    "1&0\n",
    "\\end{array}\\right)$ "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Question**: Calculate\n",
    "$$\\left(\\begin{array}{ccc}\n",
    "1&2&3\\\\\n",
    "4&5&6\\\\\n",
    "7&8&9\n",
    "\\end{array}\\right)\\circledast \\left(\\begin{array}{cc}\n",
    "1&0\\\\\n",
    "0&1\n",
    "\\end{array}\\right)$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Convolution with several input channels**\n",
    "\n",
    "In general an image (e.g. RGB) consists of several matrices (channels) stacked on top of each other = $3$-dimensional tensor. \n",
    "<br></br><img src=\"../../assets/Image_Cat_Colour.jpg\" alt=\"Image_Cat_Colour.jpg\" style=\"width: 150px\"/>\n",
    "<img src=\"../../assets/Image_Cat_Red.png\" alt=\"Image_Cat_Red.png\" style=\"width: 150px\"/><img src=\"../../assets/Image_Cat_Green.png\" alt=\"Image_Cat_Green.png\" style=\"width: 150px\"/><img src=\"../../assets/Image_Cat_Blue.png\" alt=\"Image_Cat_Blue.png\" style=\"width: 150px\"/><br></br>\n",
    "\n",
    "One filter then also consists of the same number of channels $\\mathbf{W}=(W_{ijc})$. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**2D-Convolution for $C$ input channel, 1 kernel Conv2D(C, 1)**:\n",
    "- for each image channel, there is one filter channel\n",
    "- per channel, apply the above convolution\n",
    "- sum up the resulting matrices for all channels.\n",
    "- output: ONE matrix = one channel.\n",
    "Also, one can allow a bias term $b$. Formula:\n",
    "$$(\\mathbf{X}\\circledast\\mathbf{W})_{ij}=b+ \\sum_{c=1}^C\\sum_{h=0}^{H_W-1}\\sum_{b=0}^{B_W-1}x_{i+h,j+b, c}\\cdot w_{h,b,c}$$\n",
    "\n",
    "<img src=\"../../assets/conv_one_filter.jpg\" style=\"width:300pt\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "One kernel learns one feature $\\rightarrow$ use several filters to learn several features in one step:\n",
    "\n",
    "**2D-Convolution for $C$ input channels with $D$ filters/kernels Conv2D(C, D)**:\n",
    "-for each kernel, apply Conv2D(C, 1) to get one output channel\n",
    "- stack the channels on top of each other to get $D$ output channels\n",
    "\n",
    "<img src=\"../../assets/convolution_input_C_output_D.jpg\" style=\"width:370pt\"/><img src=\"../../assets/Image_CNN_Convolution_channels_filters.png\" alt=\"Convolution with several channels and filters\" style=\"width:300pt\" title = \"Murphy\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Question:** Calculate $\\mathbf{X}\\circledast\\mathbf{W}$ for \n",
    "$$X_1=\\left(\\begin{array}{ccc}\n",
    "1&2&3\\\\\n",
    "4&5&6\\\\\n",
    "7&8&9\n",
    "\\end{array}\\right), X_2= \\left(\\begin{array}{ccc}\n",
    "0&1&2\\\\\n",
    "3&4&5\\\\\n",
    "6&7&8\n",
    "\\end{array}\\right), W_1=\\left(\\begin{array}{ccc}\n",
    "1&2\\\\\n",
    "3&4\n",
    "\\end{array}\\right), W_2=\\left(\\begin{array}{ccc}\n",
    "0&1\\\\\n",
    "2&3\n",
    "\\end{array}\\right)$$\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Convolution Layer: Dimensions** ($C_{in}$, $C_{out}$, `filter_size`=$f$):\n",
    "\n",
    "|<div style=\"width:200px\">input</div>|<div style=\"width:200px\">parameters</div>|<div style=\"width:200px\">output</div>|\n",
    "|:---|:---|:-------------|\n",
    "|$(H,B,C_{in})$-tensor<br><br>|- $C_{out}$ learnable filter tensors $W^{(d)}$ of size $f\\times f\\times C_{in}$ <br>- (possibly) $C_{out}$ bias terms $b_1,\\ldots, b_{C_{out}}$<br><br>|$$(H-f+1)\\times (B-f+1)\\times C_{out}-\\text{tensor}$$ <br> d-th channel: $b_d+\\mathbf{X}\\circledast W^{(d)}$|\n",
    "\n",
    "\n",
    "<br> <img src=\"../../assets/convolution_input_C_output_D.jpg\" style=\"width:300pt\"/>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Question:** one output channel, three input channels, filter_size=7\n",
    "\n",
    "<img src=\"../../assets/Image_Weights_for_Neuron_CNN_Gavves.png\" alt=\"Gavves, UVA Deep Learning Course\" style=\"width: 400px\"/>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Question:** 3 input channels, five output channels i.e. filters, filter_size= 7\n",
    "\n",
    "<img src=\"../../assets/Image_Weights_CNN_Gavves_more_filters.png\" alt=\"Gavves, UVA Deep Learning Course\" style=\"width: 500px\"/>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.1.3. Variants of Convolution\n",
    "\n",
    "**Padding and Same convolution: To keep size equal**\n",
    "\n",
    "If you stack several convolution layers, the output gets smaller and smaller. \n",
    "<br></br><img src=\"../../assets/Image_CNN_Images_get_smaller_Gavves.png\" alt=\"Gavves, UVA Deep Learning Course\" style=\"width: 500px\"/><br></br>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If you don't want that, use **zero padding**: Surround the image with 0's and apply convolution to this bigger matrix. \n",
    "\n",
    "If you add $p_H$ rows at top and bottom and $p_B$ columns to the sides, you get an output of size \n",
    "\n",
    "$(?)\\times (?)$. \n",
    "\n",
    "If $p_H=(H_W−1)/2$ and $p_B=(B_W−1)/2$ $\\Rightarrow$ output-size = input-size. This is called **Same Convolution**.\n",
    "\n",
    "<img src=\"../../assets/Image_CNN_Padding.png\" alt=\"convolution\" style=\"width: 400px\" title = \"Murphy\"/>   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Question:**\n",
    "Apply same convolution to the matrix\n",
    "\n",
    "$\\left(\\begin{array}{cccc}\n",
    "0&1&0&0\\\\\n",
    "1&1&1&0\\\\\n",
    "0&1&0&0\\\\\n",
    "0&0&0&0\n",
    "\\end{array}\\right)$\n",
    "with the filter \n",
    "$\\left(\\begin{array}{ccc}\n",
    "0&1&0\\\\\n",
    "1&1&1\\\\\n",
    "0&1&0\n",
    "\\end{array}\\right)$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Convolution with Stride: to reduce size more quickly**\n",
    "\n",
    "**Strided Convolution** with **stride** $s$ is convolution applied to every $s$th window. \n",
    "Output size: ?\n",
    "\n",
    "**Attention:** always choose $s$ so the above dimensions are positive integers!\n",
    "\n",
    "<img src=\"../../assets/Image_CNN_Stride.png\" alt=\"convolution\" style=\"width: 400px\" title = \"Murphy\"/>  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Dilated convolution: to reduce size more quickly**\n",
    "\n",
    "Often, pixel values of two neighboring pixels can be quite similar. Idea: Put the filter not over each pixel, but only every second or third pixel. \n",
    "\n",
    "**Dilated convolution with dilation factor $d$** only considers every $d$th input element when performing convolution (equivalent to convolution with filter-size = $(d-1)$*actual-filter-size+1).\n",
    "\n",
    "<img src=\"../../assets/Image_Dilation_Murphy.png\" alt=\"dilated convolution\" style=\"width: 500px\" title = \"Murphy\"/>  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Transpose convolution = Deconvolution: to make input bigger**\n",
    "\n",
    "**Deconvolution** is the \"opposite\" of convolution. Multiply each matrix entry with the entire filter and put the result in a window of a bigger matrix, fill the rest up with 0's. Add up all the resulting matrices:\n",
    "\n",
    "<img src=\"../../assets/Image_CNN_Transpose_Convolution.png\" alt=\"convolution\" style=\"width: 500px\" title = \"Murphy\"/>   \n",
    "\n",
    "```python\n",
    "def trans_conv(X, F):\n",
    "    h, w = F.shape\n",
    "    Y = zeros((X.shape[0] + h - 1, X.shape[1] + w - 1))\n",
    "    for i in range(X.shape[0]):\n",
    "        for j in range(X.shape[1]):\n",
    "            Y[i:i + h, j:j + w] += X[i, j] * F\n",
    "    return Y\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**3d- and 1d-convolution**\n",
    "\n",
    "The method of convolution for images can be naturally extended to tensors of all sizes by the same method: \n",
    "Input = $C$ channels of a $n-$dimensional tensor\n",
    "Filter = $C$ channels of a $n$-dimensional tensor (e.g. in 1-dim: a vector, in 3-dim: a cube)\n",
    "let filter slide over all positions of the input, multiply entries \"on top of\" each other and sum all of them up. \n",
    "\n",
    "E.g. **1D convolution:**\n",
    "\n",
    "<br></br><img src=\"../../assets/Image_CNN_Convolution_1d.png\" alt=\"1d Convolution\" style=\"width: 500px\" title = \"Murphy\"/>   <br></br>\n",
    "\n",
    "3D convolution can for example be used on 3D-data like LIDAR point clouds in autonomous driving or CAD-models...\n",
    "However: 3D convolution is computationally expensive!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Stacking Convolution Layers: Receptive Field**\n",
    "\n",
    "**Definition:** The pixels in the pre-image of one neuron inside a Convolution NN is called its **receptive field**. \n",
    "\n",
    "**Question:** Depict what happens if we stack 3 $2\\times 2$-filters on top of each other vs using one $4\\times 4$-filter. What's the number of parameters in each case?\n",
    "\n",
    "<br></br><img src=\"../../assets/Image_Convolution_Receptive_Field_Depth.jpg\" alt=\"Image_Convolution_Receptive_Field_Depth.jpg\" style=\"width: 400px\"/><br></br>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Convolutional Layers in Code**\n",
    "\n",
    "#### with Keras\n",
    "The option `filters` is the number of output channels: `keras.layers.Conv2D(filters=2, kernel_size=7)`. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The output is a 4D tensor (batch size, height, width, channels). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### with Pytorch\n",
    "\n",
    "Like all layers, you can access 2D-Convolution layers via `nn`: `nn.Conv2d(c_in, c_out, kernel_size=kernel_size)`\n",
    "1D or 3D Convolution layers similarly. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.1.4. Pooling Layers \n",
    "\n",
    "Aim: Reduce parameters, local translation invariance \n",
    "\n",
    "- Two adjacent windows overlap $\\Rightarrow$ similar values $\\Rightarrow$ don't keep all, but aggregate e.g. a 2x2 window \n",
    "- ways of aggragation: average or maximum.\n",
    "- aggregating makes the NN forget the exact positions (translation inv.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Definition:** A **max-pooling layer** (resp. **avg-pooling layer**) of a=`filter_size` $\\in \\mathbb{N}$ divides each channel of the input matrix into $a\\times a$-windows and aggregates the values in each such window to the maximum value (resp. the average). \n",
    "\n",
    "**Examples:** \n",
    "\n",
    "<img src=\"../../assets/Image_Maxpool2.jpg\" alt=\"Image_Maxpool2.jpg\" style=\"width: 400px\"/>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br></br><img src=\"../../assets/Image_CNN_MaxPooling.png\" alt=\"convolution\" style=\"width: 400px\" title = \"Murphy\"/>  \n",
    "\n",
    "**Good Practice:** MaxPooling works better\n",
    "\n",
    "**Global Average Pooling**: replace an entire channel by one single neuron by computing the average of all entries in the channel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Keras:\n",
    "max_pool = keras.layers.MaxPool2D(pool_size=2)\n",
    "avg_pool = keras.layers.AvgPool2D(pool_size=2)\n",
    "global_avg_pool = keras.layers.GlobalAvgPool2D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Pytorch:\n",
    "torch.nn.MaxPool2d(kernel_size)\n",
    "torch.nn.AvgPool2d(kernel_size)\n",
    "# no separate layer for global average pooling, just use:\n",
    "torch.nn.AvgPool2d(kernel_size= image-size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.1.5. Architecture of Convolution Neural Networks\n",
    "\n",
    "\n",
    "<img src=\"../../assets/Image_CNN_Architecture_Simple.jpg\" alt=\"Image_CNN_Architecture_Simple.jpg\" style=\"width: 500px\"/><br></br>\n",
    "\n",
    "Similarly, one could define a **Deconvolution Network (Deconv Net)** with deconvolution instead of convolution. \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "**Good practices for the architecture:** \n",
    "- use several convolution layers after each other to increase the receptive field\n",
    "- filter size: $< 11$, modern architectures often use $3$ (fewer parameters) but stack several (see receptive field)\n",
    "- then a non-linearity; most popular: ReLU\n",
    "- then pooling: most populuar: max pooling\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How did we solve the initial three aims? \n",
    "1. spatial structure information  + translation invariance: ?\n",
    "2. huge input dimensionalities/many parameters: ?\n",
    "3. Translation invariance: ?\n",
    "\n",
    "In reality the architectures are more complicated and far deeper. We will see examples in the Computer Vision Section later on. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Example: CNNs in Practice**\n",
    "\n",
    "Let's train a Convolutional Neural Network (CNN) in Keras on the MNIST fashion dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train, X_valid = X_train_full[:-5000], X_train_full[-5000:]\n",
    "y_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]\n",
    "\n",
    "X_mean = X_train.mean(axis=0, keepdims=True)\n",
    "X_std = X_train.std(axis=0, keepdims=True) + 1e-7\n",
    "X_train = (X_train - X_mean) / X_std\n",
    "X_valid = (X_valid - X_mean) / X_std\n",
    "X_test = (X_test - X_mean) / X_std\n",
    "\n",
    "X_train = X_train[..., np.newaxis]\n",
    "X_valid = X_valid[..., np.newaxis]\n",
    "X_test = X_test[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# you do not need partial, but if you use the same options in many layers, it can help save time\n",
    "# partial simply \"freezes\" some of the options in a function so you don't have to repeat yourself\n",
    "from functools import partial\n",
    "\n",
    "#the following is like creating a keras.layers.Conv2D layer with fixed options \n",
    "# kernel_size=3, activation='relu', padding=\"SAME\"\n",
    "\n",
    "DefaultConv2D = partial(keras.layers.Conv2D,\n",
    "                        kernel_size=3, activation='relu', padding=\"SAME\")\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    DefaultConv2D(filters=64, kernel_size=7, input_shape=[28, 28, 1]),\n",
    "    keras.layers.MaxPooling2D(pool_size=2),\n",
    "    DefaultConv2D(filters=128),\n",
    "    DefaultConv2D(filters=128),\n",
    "    keras.layers.MaxPooling2D(pool_size=2),\n",
    "    DefaultConv2D(filters=256),\n",
    "    DefaultConv2D(filters=256),\n",
    "    keras.layers.MaxPooling2D(pool_size=2),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(units=128, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(units=64, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(units=10, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))\n",
    "score = model.evaluate(X_test, y_test)\n",
    "X_new = X_test[:10] # pretend we have new images\n",
    "y_pred = model.predict(X_new)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.1.6 Case Study: AlexNet\n",
    "\n",
    "See [AlexNet](https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)\n",
    "\n",
    "\n",
    "<br></br><img src=\"../../assets/Image_AlexNet.png\" alt=\"AlexNet\" style=\"width:500px\"/><br></br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"../../assets/Image_AlexNet_Version_1.png\" alt=\"AlexNet Version 1\" style=\"width:500px\"/><br></br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../../assets/Image_AlexNet_Version_2.png\" alt=\"AlexNet Version 2\" style=\"width:500px\"/><br></br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"../../assets/Image_AlexNet_Version_3.png\" alt=\"AlexNet Version 3\" style=\"width:500px\"/><br></br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../../assets/Image_AlexNet_Version_4.png\" alt=\"AlexNet Version 4\" style=\"width:500px\"/><br></br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "\n",
    "<img src=\"../../assets/Image_AlexNet_Version_5.png\" alt=\"AlexNet Version 5\" style=\"width:500px\"/><br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Homework:** Compute the dimensions and number of parameters for all layers in the following CNN: \n",
    "- input: 3 channels size 32x32\n",
    "- 1st Layer: SAME Convolution, 8 kernels of size 3, stride 2  \n",
    "- 2nd layer: Convolution, padding = 1, 16 kernels of size 3, stride = 2\n",
    "- 3rd layer: maxpool of size 2x2\n",
    "- 4th layer: global average pooling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
