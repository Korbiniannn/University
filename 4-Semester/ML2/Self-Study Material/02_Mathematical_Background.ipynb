{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Mathematical Background for Deep Learning\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Algebra (remember, remember Mathe 1...)\n",
    "\n",
    "\n",
    "A **scalar** is a single number, e.g. an integer, real number... denoted by a non-bold italic letter $a, n, x$. \n",
    "\n",
    "A **vector** is an element of a vector space = a 1D-array of numbers, denoted by a bold letter, e.g. \n",
    "$$\\mathbf{x}=\\left(\\begin{array}{c}\n",
    "x_1\\\\\n",
    "\\vdots\\\\\n",
    "x_n\n",
    "\\end{array}\\right).$$ \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "A **matrix** is a 2D-array of numbers correpsonding to a linear map between vector spaces, and denoted by a large bold letter, e.g. \n",
    "$$\\mathbf{W}=\\left(\\begin{array}{cccc}\n",
    "w_{11}&w_{12}&\\ldots&w_{1n}\\\\\n",
    "\\vdots&\\ddots&\\ddots& \\vdots\\\\\n",
    "w_{m1}&w_{m2}&\\ldots &w_{mn}\n",
    "\\end{array}\\right).$$ \n",
    "It gives rise to the linear map from $\\mathbb{R}^n$ to $\\mathbb{R}^m$ given by \n",
    "$$\n",
    "\\mathbf{W}\\cdot \\mathbf{x}=\\left(\\begin{array}{c}\n",
    "w_{11}\\cdot x_1+w_{12}\\cdot x_2+\\ldots+w_{1n}\\cdot x_n\\\\\n",
    "\\vdots \\\\\n",
    "w_{m1}\\cdot x_1+w_{m2}\\cdot x_2+\\ldots +w_{mn}\\cdot x_n\n",
    "\\end{array}\\right)\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A **tensor** is a possibly higher-dimensional abstraction of matrices: it is an n-dimensional array of numbers, e.g. \n",
    "- zero-dimensional tensor = scalar\n",
    "- one-dimensional tensor = vector\n",
    "- two-dimensional tensor = matrix\n",
    "- three dimensional tensor = a cube of numbers..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Matrices and systems of linear equations\n",
    "\n",
    "An equation of the form \n",
    "\n",
    "$$\\mathbf{W}\\cdot \\mathbf{x}=\\mathbf{b}$$\n",
    "is a system of linear equations (LGS)\n",
    "$$\\begin{array}{ccc}\n",
    "w_{11}\\cdot x_1+w_{12}\\cdot x_2+\\ldots+w_{1n}\\cdot x_n&=&b_1\\\\\n",
    "\\vdots&& \\vdots\\\\\n",
    "w_{m1}\\cdot x_1+w_{m2}\\cdot x_2+\\ldots +w_{mn}\\cdot x_n&=&b_n\n",
    "\\end{array}.$$\n",
    "\n",
    "\n",
    "Such a system can have no solution, exactly one solution (if the matrix $\\textbf{W}$ is invertible, i.e. has full rank), or infinitely many solutions (if the matrix $\\textbf{W}$ is not invertible, i.e. has rank $<n$$). (GauÃŸ Algorithm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Matrix multiplication\n",
    "\n",
    "Remember that matrices $\\mathbf{A}=(a_{ij})$, $\\mathbf{B}=(b_{ij})$ can be multiplied as long as the number of columns of $A$ is the same as the number of rows of $B$: \n",
    "$\\mathbf{A}\\mathbf{B}=(\\sum_{k}a_{ik}b_{kj})_{ij}$, e.g.\n",
    "$$\\left(\\begin{array}{cc}\n",
    "1&2\\\\\n",
    "3&4\\\\\n",
    "5&6\n",
    "\\end{array}\\right)\\cdot \\left(\\begin{array}{ccc}\n",
    "1&1&-1\\\\\n",
    "0&-1&1\n",
    "\\end{array}\\right)=?$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Answer:**\n",
    "$\\left(\\begin{array}{ccc}\n",
    "1&-1&1\\\\\n",
    "3&-1&1\\\\\n",
    "5&-1&1\n",
    "\\end{array}\\right)$\n",
    "\n",
    "**Question:** The neutral element for multiplication of $n\\times n$-matrices is ?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Answer:** the **identity matrix**\n",
    "$\\mathbb{I}_n=\\left(\\begin{array}{cccc}\n",
    "1&0&\\ldots&0\\\\\n",
    "0&1&\\ldots &0\\\\\n",
    "\\vdots&\\ddots&\\vdots &0\\\\\n",
    "0&0&\\ldots & 1\n",
    "\\end{array}\\right)$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Invertible matrices\n",
    "\n",
    "If the matrix $\\mathbf{W}$ is square (i.e. $n\\times n$) and has **full rank** (meaning the column vectors are linearly independent, i.e. they span a vector space of dimenion $n$ $\\Leftrightarrow$ none of the eigenvalues is 0), the corresponding system of linear equations\n",
    "$$\\mathbf{W}\\mathbf{x}=\\mathbf{b}$$ \n",
    "has exaclty one solution $\\mathbf{x}$. This solution can be recovered by multiplying $\\mathbf{b}$ with the unique inverse matrix $\\mathbf{W}^{-1}$ (i.e. $\\mathbf{x}=\\mathbf{W}^{-1}\\mathbf{b}$), which satisfies \n",
    "$$\\textbf{W}^{-1}\\cdot \\textbf{W}=\\textbf{W}\\cdot \\textbf{W}^{-1}=\\mathbb{I}_n.$$ \n",
    "\n",
    "Matrices with rank less than the number of rows (meaning at least one of the eigenvalues is 0) are called **singular**. They cannot be inverted. \n",
    "\n",
    "Note: the inverse matrix is good in theory, but not so much in pracitce. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem with inverting invertible matrices numerically (i.e. with a computer):**\n",
    "\n",
    "1. Numerical instability: Computers store and calculate in floating numbers $\\Rightarrow$ small rounding errors. This can lead to huge differences when inverting a matrix. Example: $1\\times 1$-matrix $\\epsilon_1$ = very very small number. \n",
    "\n",
    "exact inverse: $\\frac{1}{\\epsilon_1}$= a very large number. \n",
    "\n",
    "if there was a floating number rounding error in the calculation, and we don't end up with $\\epsilon_1$, but with $2\\epsilon_1$: $\\frac{1}{2}\\frac{1}{\\epsilon_1}$\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "2. Memory advantage: \n",
    "$$\\mathbf{A}=\\left(\\begin{array}{ccc}\n",
    "1  &  0  &   2\\\\\n",
    "-1 &  5  &   0\\\\\n",
    "0  &  3  &  -9\n",
    " \\end{array}\\right)\\Rightarrow \\mathbf{A}^{-1}=\\left(\\begin{array}{ccc}\n",
    "0.8824 &  -0.1176  &  0.1961\\\\\n",
    "0.1765 &   0.1765  &  0.0392\\\\\n",
    "0.0588 &   0.0588  &  -0.0980\\\\\n",
    "\\end{array}\\right)$$\n",
    "$$\\Rightarrow \\mathbf{A}\\cdot \\mathbf{A}^{-1}=\\left(\\begin{array}{ccc}\n",
    "1.0000  &  0.0000   &-0.0000\\\\\n",
    "0       &  1.0000   &  -0.0000\\\\\n",
    "0       & -0.0000   &  1.0000 \\end{array}\\right)\\neq I_n$$\n",
    "Due to rounding errors, the resulting matrix is not the identity matrix but has very small float numbers instead of 0's - while 0's aren't neccessarily stored in memory, these small entries are!\n",
    "\n",
    "$\\Rightarrow$ Never invert a matrix $\\mathbf{A}$ numerically if you can help it! (Rather solve the linear equation $\\mathbf{A}\\mathbf{x}=\\mathbf{b}$ in a different way. (Cholesky decomposition))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Transpose of a matrix\n",
    "\n",
    "The transpose of a matrix $\\mathbf{W}$ as above is the matrix you get by the mirror image across the main diagonal: \n",
    "$$\\mathbf{W}^T=\\left(\\begin{array}{cccc}\n",
    "w_{11}&w_{21}&\\ldots&w_{m1}\\\\\n",
    "\\vdots&\\ddots&\\ddots& \\vdots\\\\\n",
    "w_{1n}&w_{2n}&\\ldots &w_{mn}\n",
    "\\end{array}\\right).$$\n",
    "\n",
    "Example: \n",
    "$$\\left(\\begin{array}{cc}\n",
    "1&2\\\\\n",
    "3&4\n",
    "\\end{array}\\right)^T=\\left(\\begin{array}{cc}\n",
    "1&3\\\\\n",
    "2&4\n",
    "\\end{array}\\right)$$\n",
    "Rule: $(AB)^T=A^TB^T$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Special matrices and vectors: \n",
    "\n",
    "- unit vector $||\\mathbf{x}||=1$. \n",
    "- symmetric matrix: $\\mathbf{W}=\\mathbf{W}^T$\n",
    "- orthogonal matrix: $$\\mathbf{W}\\cdot \\mathbf{W}^T=\\mathbf{W}^T\\cdot \\mathbf{W}=I_n\\Rightarrow \\mathbf{W}^{-1}=\\mathbf{W}^T $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Differentiation\n",
    "\n",
    "\n",
    "### Differentiating real-valued functions in one variable\n",
    "\n",
    "School/Mathe2: If $f(x)$ is a real-valued function in one real variable which is \"smooth\" enough, one can compute the **derivative** \n",
    "$f'(x)=\\frac{d}{dx}f(x)$, which is the slope of the function at the point $x$. \n",
    "\n",
    "**Question:** $f(x)=x^2 + \\sin x + x$. Compute the derivative!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Answer:** $f'(x)=2x+\\cos x +1$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Rules for differentiating:** \n",
    "- Chain rule: $$(f\\circ g)'(x)=f'(g(x))\\cdot g'(x)$$\n",
    "- Product rule: $$(f(x)\\cdot g(x))'=f'(x)\\cdot g(x)+ f(x)\\cdot g'(x)$$\n",
    "- Quotient rule: \n",
    "  $$\\left(\\frac{f(x)}{g(x)}\\right)'=\\frac{f'(x)\\cdot g(x)- f(x)\\cdot g'(x)}{(g(x))^2}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Example:** $f(x)=x\\cdot \\log x + e^{2x}$. Compute the derivative using chain rule and multiplication rule!\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Answer:** $f'(x)=1\\cdot\\log x + x\\cdot \\frac{1}{x} +2\\cdot e^{2x}$\n",
    "\n",
    "**Example:** $\\sigma(x)=\\frac{1}{1+e^{-x}}$ is the sigmoid function. Compute the derivative using the chain rule!\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Answer:** $\\sigma'(x)=-\\frac{1}{(1+e^{-x})^2}\\cdot (-e^{-x})=\\frac{1}{1+e^{-x}}\\cdot \\frac{-e^{-x}}{(1+e^{-x})}=\\sigma(x)\\cdot (1-\\sigma(x))$. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Real-Valued Functions in several variables\n",
    "\n",
    "Now we consider \"smooth\" real-valued functions $f(x_1,\\ldots, x_n)$ in $n$ variables(one-dimensional output, n-dimensional input). \n",
    "\n",
    "**Example:** Atmospheric pressure (Luftdruck) at a certain time is a function of longitude and latitude. What does the graph of this function (i.e. plotting atmospheric pressure versus longitude and latitude in 3D) look like?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Answer:** It is a smooth surface in 3D with hills and valleys. And on this surface one can consider lines of constant pressure.  \n",
    "\n",
    "<br></br><img src=\"./Other_Images/Image_Isobaren.png\" alt=\"Isobaren\" width=\"400\" title = \"Wikipedia\"/>   <br></br>\n",
    "\n",
    "For a function as above, one calls the sets of points with constant value $f(x_1,\\ldots, x_n)=c$ the **level sets** of a function. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Example:** The prediction function of a NN with one output head is a (mostly) smooth real-valued function (whether entirely smooth or not depends on the activation functions) in the input and the weights of the NN.\n",
    "\n",
    "**Most important example**: The loss function of a ML model (e.g. MSE) is a real-valued smooth function in the parameters."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Partial differentials and Gradient\n",
    "\n",
    "If $f(x_1,\\ldots, x_n)$ is a smooth real-valued function in $n$ variables as above, one can compute the **partial differential** with respect to each of the variables $x_i$ which is denoted by \n",
    "$$\\frac{\\partial}{\\partial x_i}f(x_1,\\ldots, x_n)$$\n",
    "This is EXACTLY the same as \"normal\" differentiation: we assume the other variables are constants and consider $f(x_1,\\ldots, x_n)$ only as a function in $x_i$, and compute the derivative as above!\n",
    "\n",
    "**Example:** $f(x,y,z)=x^2+y^2+z\\cdot \\sin x$. Compute the partial derivatives with respect to $x,y,z$!\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Answer:** $\\frac{\\partial}{\\partial x}f(x,y,z)=2x+z\\cdot \\cos x, \\frac{\\partial}{\\partial y}f(x,y,z)=2y, \\frac{\\partial}{\\partial z}f(x,y,z)=\\sin x$. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Example:** $f(x,y,z)=\\text{Softmax}_x(x,y,z)=\\frac{e^x}{e^x+e^y+e^z}$. Compute the partial derivatives with respect to $x,y,z$!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Answer:** \n",
    "$$\\frac{\\partial}{\\partial x}f(x,y,z)=\\frac{e^x\\cdot (e^x+e^y+e^z)- e^x\\cdot e^x}{e^x+e^y+e^z}=\\frac{e^x}{e^x+e^y+e^z}\\cdot(1- \\frac{e^x}{e^x+e^y+e^z})=f(x,y,z)\\cdot \\left(1-f(x,y,z)\\right),$$\n",
    "$$\\frac{\\partial}{\\partial y}f(x,y,z)=e^x\\cdot (-\\frac{1}{(e^x+e^y+e^z)^2})\\cdot e^y=-\\frac{e^xe^y}{(e^x+e^y+e^z)^2},$$\n",
    "$$\\frac{\\partial}{\\partial z}f(x,y,z)=-\\frac{e^xe^z}{(e^x+e^y+e^z)^2}.$$ "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The **gradient** $\\nabla f$ of $f$ is the vector of all partial differentials:\n",
    "$$\\nabla_{x_1,\\ldots,x_n} f(x_1,\\ldots, x_n) =\\left(\\begin{array}{c}\n",
    "\\frac{\\partial}{\\partial x_1}f\\\\\n",
    "\\frac{\\partial}{\\partial x_2}f\\\\\n",
    "\\vdots\\\\\n",
    "\\frac{\\partial}{\\partial x_n}f\n",
    "\\end{array}\\right)$$\n",
    "If it is clear for which variables we compute the partial differentials, one can also drop the subscript in the gradient sign $\\nabla$.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Theorem:** $\\nabla f(x_1,\\ldots, x_n)$ ALWAYS points in the direction of the steepest ascent of $f$ at the point $(x_1,\\ldots ,x_n)$ and $-\\nabla f(x_1,\\ldots, x_n)$ in the direction of the steepest descent. \n",
    "\n",
    "**Example:** Compute the gradient of the function $f(x,y,z)=x^2+y^2+z\\cdot \\sin x$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Answer:** \n",
    "$$\\nabla f(x,y,z)=\\left(\\begin{array}{c}\n",
    "2x+z\\cdot \\cos x\\\\\n",
    "2y\\\\\n",
    "\\sin x\n",
    "\\end{array}\\right).$$ "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Differentiating real-valued higher-dimensional functions \n",
    "\n",
    "If $f(x)=(f_1(x),\\ldots, f_d(x))$ is a vector of real differentiable functions in one variable (one-dimensional input, $n$-dimensional output), one can compute the differential by $x$ as: \n",
    "\n",
    "$f'(x)=(f'_1 (x),\\ldots, f'_d(x))$. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Differentiating higher-dimensional functions in several variables: The Jacobian\n",
    "\n",
    "Now we consider  $f(x_1,\\ldots, x_n)=\\left(f_1 (x_1,\\ldots, x_n),\\ldots, f_d(x_1,\\ldots, x_n)\\right)$ be a vector of real differentiable functions in $n$ variables ($n$-dimensional input, $d$-dimensional output). \n",
    "\n",
    "**Question:** What is the partial differential of of $f$ by $x_j$?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Answer:** an entire vector:\n",
    "$\\frac{\\partial}{\\partial x_i}f(x_1,\\ldots, x_n)=\\left(\\frac{\\partial f_1 (x)}{\\partial x_i},\\ldots, \\frac{\\partial f_d (x)}{\\partial x_i}\\right).$\n",
    "\n",
    "**Question:** What would an equivalent of the gradient (i.e. a tensor with all partial differentials) for real-valued functions look like in this case?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Answer:** something two-dimensional, i.e. a matrix!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The **Jacobian matrix** is defined as the matrix which has the partial differentials of the function $f$ as vectors, (and gradients of the functions $f_i$ as rows): \n",
    "$$Jf_{x_1,\\ldots, x_n}(x_1,\\ldots, x_n)=\\left(\\begin{array}{cccc}\n",
    "\\frac{\\partial f_1 (x)}{\\partial x_1} & \\frac{\\partial f_1 (x)}{\\partial x_2}&\\cdots & \\frac{\\partial f_1 (x)}{\\partial x_n}\\\\\n",
    "\\vdots&\\vdots&\\ddots&\\vdots\\\\\n",
    "\\frac{\\partial f_d (x)}{\\partial x_1} & \\frac{\\partial f_1 (x)}{\\partial x_2}&\\cdots & \\frac{\\partial f_d (x)}{\\partial x_n}\n",
    "\\end{array}\\right)$$\n",
    "If it is clear what the variables are (usually all of them), one can also drop the subscript. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Example:** \n",
    "$f(x,y,z)=\\left(\\begin{array}{c}\n",
    "2x+ 3y + 4z\\\\\\\n",
    "x -y -2z\\\\\n",
    "y-z\n",
    "\\end{array}\\right)$\n",
    "\n",
    "Write $f$ in matrix notation and compute the Jacobian of $f$!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Answer:** \n",
    "$f(x,y,z)=\\left(\\begin{array}{ccc}\n",
    "2& 3& 4\\\\\\\n",
    "1 &-1 &-2\\\\\n",
    "0& 1&-1\n",
    "\\end{array} \\right)\\cdot \\left(\\begin{array}{c}\n",
    "x\\\\\\\n",
    "y\\\\\n",
    "z\n",
    "\\end{array} \\right)\n",
    "$\n",
    "\n",
    "$f_1=2x+ 3y + 4z, f_2=x -y -2z, f_3=y-z$\n",
    "\n",
    "$\n",
    "\\Rightarrow Jf=\\left(\\begin{array}{ccc}\n",
    "2& 3& 4\\\\\\\n",
    "1 &-1 &-2\\\\\n",
    "0& 1&-1\n",
    "\\end{array} \\right)\n",
    "$\n",
    "\n",
    "So the Jacobian of a linear function (multiplication with a matrix) is the matrix itself!\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Example:** Compute the Jacobian of the following function: \n",
    "$Jf=\\left(\\begin{array}{c}\n",
    "x^2+\\log y+z^2 +xy\\\\\n",
    "y\\cdot e^x \n",
    "\\end{array}\\right)$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Answer: \n",
    "$Jf=\\left(\\begin{array}{ccc}\n",
    "2x+y&\\frac{1}{y}+x&2z\\\\\n",
    "ye^x& e^x& 0\n",
    "\\end{array}\\right)$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Chain Rule for Differentiation\n",
    "\n",
    "If $f\\colon \\mathbb{R}^n\\to \\mathbb{R}^m$ (input n-dimensional, output m-dimensional) and $g\\colon \\mathbb{R}^m\\to \\mathbb{R}^k$ (input m-dimensional, output k-dimensional) are two smooth functions, we can consider the composite $g\\circ f\\colon \\mathbb{R}^n\\to \\mathbb{R}^k$. \n",
    "\n",
    "**Chain Rule for Differentiation:** $$J(g\\circ f)(x_1,\\ldots, x_n)=Jg(f(x_1,\\ldots, x_n))\\cdot Jf(x_1,\\ldots, x_n)$$\n",
    "If $k=1$, $g$ and $g\\circ f$ are real-valued functions, so we can compute their gradient. Then, as a special case of the above, we get the chain rule: \n",
    "$$\\nabla (g\\circ f)(x_1,\\ldots, x_n)=\\nabla g(f(x_1,\\ldots, x_n))\\cdot Jf(x_1,\\ldots, x_n)$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Notational convention\n",
    "\n",
    "Since it is difficult to always think about the dimensions, i.e. whether you deal with ordinary differentials, the gradient, or the Jacobian, especially when dealing with large chains of functions, one writes \n",
    "\n",
    "$\\frac{\\partial f}{\\partial \\mathbf{x}}$ for all kinds of functions $f$ and numbers of parameters in the vector $\\mathbf{x}$, i.e.\n",
    "- if $f$ has 1-dim'l output and the input $\\mathbf{x}=x$ is real, $\\frac{\\partial f}{\\partial \\mathbf{x}}=f'(x)$.\n",
    "- if $f$ has 1-dim'l output and the input $\\mathbf{x}=(x_1,\\ldots,x_n)$ is n-dim'l, $\\frac{\\partial f}{\\partial \\mathbf{x}}=\\nabla f(\\mathbf{x})$\n",
    "- if $f$ has m-dim'l output and the input $\\mathbf{x}=(x_1,\\ldots,x_n)$ is n-dim'l, $\\frac{\\partial f}{\\partial \\mathbf{x}}=Jf(\\mathbf{x})$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Computing powers of a matrix: vanishing or exploding entries\n",
    "\n",
    "Recall how you can compute the powers of a matrix $A$ from Mathe 1: \n",
    "Compute the eigenvalues of $A$: $\\lambda_1, \\ldots, \\lambda_P$ (P=number of parameters). \n",
    "Compute the eigenvectors of $A$: $v_1,\\ldots, v_P$ and let $B$ be the matrix with these eigenvectors as columns. If \n",
    "$D=\\left(\\begin{array}{cccc}\n",
    "\\lambda_1&0&\\ldots&0\\\\\n",
    "0&\\lambda_2&\\ldots&0\\\\\n",
    "\\ldots&\\ldots&\\ddots&\\ldots\\\\\n",
    "0&0&\\ldots&\\lambda_P\n",
    "\\end{array}\\right)$ is the diagonal matrix with the eigenvalues on the diagonal, then one can show that \n",
    "$$A=B\\cdot D\\cdot B^{-1}\\Rightarrow A^k=(B\\cdot D\\cdot B^{-1})\\cdot \\ldots \\cdot B\\cdot D\\cdot B^{-1}=B\\cdot D^k\\cdot (B^{-1})$$\n",
    "and $D^k$ is the diagonal matrix with $\\lambda^k$ on the diagonal. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$A=B\\cdot D^k\\cdot (B^{-1})$$\n",
    "and $D^k$ is the diagonal matrix with $\\lambda^k$ on the diagonal. \n",
    "\n",
    "$\\Rightarrow$ If the eigenvalues are large, then the entries of $D^k$ become huge with growing $k$ (and therefore $A^k$ as well, it \"explodes\"). \n",
    "\n",
    "$\\Rightarrow$ If the eigenvalues are less than 1, then the entries of $D^k$ become almost 0 with growing $k$ (and therefore $A^k$ as well, it \"vanishes\"). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Application in RNNs:**\n",
    "- Activations: To compute the activations after $n$ time steps, the hidden-to-hidden weight matrix $\\mathbf{W}_{hh}$ gets multiplied $n$ times (plus activations in between). $\\Rightarrow$ if the eigenvalues of $\\mathbf{W}_{hh}$ are not close to 1, the activations will either explode or vanish. \n",
    "- Backpropagation Through Time (BPTT): For each optimization step in (S)GD, we need to compute the gradient from the first to the last layer. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For those who are interested in the mathematics: \n",
    "Chain rule for the Loss $L_T$ after time step T: \n",
    "$$\\frac{\\partial L_T}{\\partial z_i}=\\frac{\\partial L}{\\partial z_T}\\cdot \\frac{\\partial h(z_{T-1})}{\\partial z_{T-1}}\\cdot \\frac{\\partial h(z_{T-2})}{\\partial z_{T-2}}\\cdot \\frac{\\partial h(z_{T-3})}{\\partial z_{T-3}}\\cdot \\ldots \\cdot \\frac{\\partial h(z_{i})}{\\partial z_{i}}$$\n",
    "Note: the Jacobians $\\frac{\\partial h(z_{i})}{\\partial z_{i}}$ are the same for all $i$! $\\Rightarrow$ If we write $Jh:=\\frac{\\partial h(z_{i})}{\\partial z_{i}}$, we get: \n",
    "\n",
    "$$\\frac{\\partial L_T}{\\partial z_i}=\\frac{\\partial L}{\\partial z_T}\\cdot J^{T-i}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\Rightarrow \\frac{\\partial L_T}{\\partial \\theta}=\\sum_{i=1}^T \\frac{\\partial L}{\\partial z_i}\\cdot \\frac{\\partial h(z_{i-1})}{\\partial \\theta}=\\sum_{i=1}^T \\frac{\\partial L}{\\partial z_T}\\cdot J^{T-i}\\cdot\\frac{\\partial h(z_{i-1})}{\\partial \\theta}$\n",
    "\n",
    "This means that the farther you go back in time, the more often the same Jacobian matrix is multiplied over and over again! \n",
    "\n",
    "$\\Rightarrow$ if the eigenvalues of the Jacobian are not close to 1, the activations will either explode or vanish. \n",
    "\n",
    "**Solution:** Stop going back in time after a fixed number of steps! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.9 (default, Apr 13 2022, 08:48:06) \n[Clang 13.1.6 (clang-1316.0.21.2.5)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
