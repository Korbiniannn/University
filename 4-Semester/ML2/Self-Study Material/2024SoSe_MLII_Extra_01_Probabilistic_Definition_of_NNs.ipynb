{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Chapter 1 Additional Material: Probabilistic Definition of NNs\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Statistics Background (Wechsel zu Deutsch, da dort die Terminologie bekannter ist):\n",
    "\n",
    "**Setting:** Bauteil-Fertigung in der Produktion\n",
    "\n",
    "Zufallsvariablen (random variables): $X=$ Abweichung in x-Richtung vom Soll-Maß, und entsprechend $Y$ und $Z$ in y- und z-Richtung. \n",
    "\n",
    "$\\Rightarrow$ $X,Y,Z$ haben als stetige Zufallsvariablen eigene Wahrscheinlichkeitsdichte (probability density) und Verteilungsfunktion (distribution).\n",
    "\n",
    "**Frage:** Sind diese Zufallsvariablen unabhängig (independent)?\n",
    "\n",
    "**Antwort:** Nein! Bei einer leichten Verdrehung des Bauteils bei der Messung gibt es in alle drei Richtungen Abweichungen, selbst wenn das Bauteil eventuell maßlich in Ordnung ist. \n",
    "\n",
    "$\\Rightarrow$ Betrachte X,Y,Z zusammen als **Zufallsvektor** $(X,Y,Z)$ mit **einer** gemeinsamen Wahrscheinlichkeitsdichte Verteilungsfunktion! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Training via MLE (Maximum Likelihood Estimate) or MAP (Maximum A Posteriori)**: \n",
    "- MLE: optimize $p(D_{\\text{train}}|\\theta)$ wrt $\\theta$, i.e. find the parameters that maximize the probability of getting the training data. \n",
    "- MAP: optimize $p(\\theta|D_{\\text{train}})$ for an assumed prior distribution $p(\\theta)$ of the parameters, i.e. find the most likely parameters given the training data. \n",
    "Formula of Bayes: \n",
    "$$p(\\theta|D_{\\text{train}})=\\frac{p(D_{\\text{train}}|\\theta)\\cdot p(\\theta)}{p(D_{\\text{train}})}$$\n",
    "\n",
    "**Inference** works by computing $p(y|x)$, and taking the most likely $y$ or sampling $y$ from the distribution.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Erinnerung: Kovarianz als Maß der Abhängigkeit von Zufallsvariablen\n",
    "\n",
    "Erinnerung: $\\text{Cov}(X,Y) = E(XY) - E(X)E(Y)$ heißt **Kovarianz** (covariance) zwischen Zufallsvariablen $X$ und $Y$ (ein Maß für die Abhängigkeit zwischen $X$ und $Y$).\n",
    "\n",
    "Sind $X, Y$ Zufallsvariablen und $a, b$ reelle Zahlen, so gelten:\n",
    "- $\\text{Cov}(X,Y) = \\text{Cov}(Y,X)$ und $\\text{Cov}(X,X) = \\text{Var}(X)$\n",
    "- $\\text{Cov}(X + a, Y + b) = \\text{Cov}(X, Y)$\n",
    "- Sind $X$, $Y$ stochastisch unabhängig, so folgt $\\text{Cov}(X,Y)=0$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "**Definition:** Seien $X_1,\\ldots, X_d$ Zufallsvariablen. Dann definiert man ihre **Kovarianzmatrix** als die $d\\times d$-Matrix\n",
    "$$ \\Sigma = \\text{Cov}(X_1,\\ldots, X_d)=\\left(\\begin{array}{cccc}\n",
    "        Var(X_1)&\\text{Cov}(X_1, X_2)&\\ldots& \\text{Cov}(X_1, X_d)\\\\\n",
    "        \\text{Cov}(X_2, X_1) &  \\text{Var}(X_2) & \\ldots &\\text{Cov}(X_2, X_d)\\\\\n",
    "        \\vdots&\\vdots &\\ddots&\\vdots\\\\\n",
    "        \\text{Cov}(X_d, X_1) & \\text{Cov}(X_d, X_2)&\\ldots&  \\text{Var}(X_d)\n",
    "    \\end{array}\\right)$$\n",
    "\n",
    "**Es gilt:** \n",
    "- $\\Sigma$ ist symmetrisch (also $\\Sigma^T=\\Sigma$), da $\\text{Cov}(X_i,X_j)=\\text{Cov}(X_j,X_i)$. \n",
    "- $\\Sigma$ ist diagonal genau dann wenn die Zufallsvariablen unabhängig sind."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Multivariate Gaußsche Normalverteilung\n",
    "\n",
    "Die bekannten (univariaten) Gaußschen Normalverteilung hat Dichte: \n",
    "$$N(x|\\mu,\\sigma^2)=\\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}\\exp\\left(-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}\\right).$$ \n",
    "\n",
    "Es gibt eine multivariate (mehrdimensionale) Verallgemeinerung: Für Zufallsvektoren $\\mathbf{X}=(X_1,\\ldots, X_n)$ mit Erwartungswert $\\mathbf{\\mu}=(\\mu_1,\\ldots, \\mu_d)$ und Kovarianzmatrix $\\Sigma$ ist die multivariate Gaußsche Normalverteilung gegeben durch die Dichte: \n",
    "$$N(\\mathbf{x}|\\mathbf{\\mu},\\Sigma)=\\frac{1}{\\sqrt{(2\\pi)^d det(\\Sigma)}}\\exp\\left(-\\frac{1}{2}(\\mathbf{x}-\\mathbf{\\mu})^T\\Sigma^{-1}(\\mathbf{x}-\\mathbf{\\mu})\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Probabilistic Formulation of NNs\n",
    "\n",
    "Just like in ML 1, the models for NNs are actually formulated as distributions: \n",
    "\n",
    "### Regression \n",
    "\n",
    "**Output of a Regression NN:** \n",
    "- univariate regression: a real number (one unit in the last layer)\n",
    "- multivariate regression: a real-valued vector of dimension $d$ ($d$ units in the last layer)\n",
    "  \n",
    "**Probabilistic Regression Model** \n",
    "If we denote the output of the NN by $o(x)$ ($\\in \\mathbb{R}$ or $\\in \\mathbb{R}^d$), we can see the output as the **expected value** of a Normal distribution. The \"real\" distribution of labels can be modeled as this Normal distribution. If we suppose that the underlying distribution of the data has constant variance $\\sigma^2$/covariance matrix $ \\Sigma$ (i.e. do not depend on the input $x$), the probability distribution learned by the model is given by: \n",
    "- for univariate regression: $p(y|x)=N(y|o(x), \\sigma^2)$, where $N$ is the univariate Gaussian\n",
    "- for multivariate regression: $p(y|x)=N(y|o(x), \\Sigma)$, where $N$ is the multivariate Gaussian.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### Classification \n",
    "\n",
    "**Output of a Classification NN:** \n",
    "A vector of probabilities $o(x)=(p_1(x),\\ldots, p_c(x))$, where $p_i(x)$ is the model's predicted probablitiy of $x$ being in class $i$. \n",
    "\n",
    "**Probabilistic Regression Model:** $p(y|x)=\\text{Cat}(y|o(x))$, where Cat is the categorical distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Training via MLE (Maximum Likelihood Estimate) or MAP (Maximum A Posteriori)**: \n",
    "- MLE: optimize $p(D_{\\text{train}}|\\theta)$ wrt $\\theta$, i.e. find the parameters that maximize the probability of getting the training data. \n",
    "- MAP: optimize $p(\\theta|D_{\\text{train}})$ for an assumed prior distribution $p(\\theta)$ of the parameters, i.e. find the most likely parameters given the training data. \n",
    "Formula of Bayes: \n",
    "$$p(\\theta|D_{\\text{train}})=\\frac{p(D_{\\text{train}}|\\theta)\\cdot p(\\theta)}{p(D_{\\text{train}})}$$\n",
    "\n",
    "**Inference** works by computing $p(y|x)$, and taking the most likely $y$ or sampling $y$ from the distribution.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "6c689652473bf5d24b7e8082146cd0e239d6c11b655d160204aff73691e33248"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
